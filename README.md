# mech_interpretability_journey
This repository is part of my personal journey to learn mechanistic interpretability of large language models (LLMs).

For now, most of the code comes from Arean’s tutorial lessons, but in the coming weeks I’ll start separating my own experiments from the tutorial materials.

If you’re interested in following my progress, I write a [daily blog](https://medium.com/@paolobiolghini) where I document what I’m learning and the insights I gain along the way.
And if you’d like to join me in this journey, feel free to reach out:  learning together is always more rewarding than learning alone.

The main roadmap I’m following is inspired by Neel Nanda’s work and his post on [how to get started in this field.](https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher)

Soon, this README will be updated with a clear plan, a list of resources, and a structured overview of my progress.
